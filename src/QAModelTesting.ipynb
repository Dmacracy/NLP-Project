{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"../models/SQuAD2_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(questions, text):\n",
    "    for question in questions:\n",
    "        inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "        # Need to pop token type ids when using distilbert because this model does not \n",
    "        # handle them, but the encoder still sets them for some reason. \n",
    "        inputs.pop('token_type_ids', None)\n",
    "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        answer_start_scores, answer_end_scores = model(**inputs)\n",
    "        \n",
    "        # Get the most likely beginning of answer with the argmax of the score\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        # Get the most likely end of answer with the argmax of the score\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1  \n",
    "\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When did The Cure form?\n",
      "Answer: 1978\n",
      "\n",
      "Question: How many prime numbers are there?\n",
      "Answer: infinitely many\n",
      "\n",
      "Question: What is my favorite color?\n",
      "Answer: red\n",
      "\n",
      "Question: how many neurons are there in my brain?\n",
      "Answer: 100 billion\n",
      "\n",
      "Question: When did Lincoln die?\n",
      "Answer: [CLS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''My favorite color is red. My favorite color is NOT blue. There are 100 billion neurons in the brain. \n",
    "There are infinitely many prime numbers. \n",
    "The Cure are an English rock band formed in Crawley, West Sussex, in 1978.'''\n",
    "questions = ['When did The Cure form?', 'How many prime numbers are there?', \n",
    "             'What is my favorite color?', \"how many neurons are there in my brain?\", 'When did Lincoln die?']\n",
    "answer_questions(questions, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerJoiner(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, hidden_dim, num_answers):\n",
    "        super(AnswerJoiner, self).__init__()\n",
    "        \n",
    "        _, self.embedding_dim = self.embeddings.shape\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_answers = num_answers\n",
    "        self.input_size = self.embedding_dim * (self.num_answers + 1)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
    "        self.W2 = nn.Linear(self.hidden_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "        \n",
    "    def forward(self, embedded_q_and_a_s, correct_answer=None):\n",
    "        embeds = self.dropout_layer(embedded_q_and_a_s)\n",
    "        pred_answer = self.W2(self.tanh(self.W1(x)))\n",
    "\n",
    "        if correct_answer is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(pred_answer, correct_answer)\n",
    "            return loss\n",
    "        else:\n",
    "            return pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embeddings, pred_answers, num_answers, correct_answers, hidden_dim, epochs, learning_rate):\n",
    "\n",
    "    model = AnswerJoiner(embeddings, hidden_dim, num_answers)\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        loss = model.forward(batch_W[b], batch_P[b], Y=batch_Y[b])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"loss: \",loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import InferSent\n",
    "model_version = 2\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "Infermodel = InferSent(params_model)\n",
    "Infermodel.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'GloVe/glove.840B.300d.txt' if model_version == 1 else 'fastText/crawl-300d-2M.vec'\n",
    "Infermodel.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "Infermodel.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Infermodel.encode([], bsize=128, tokenize=False, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
